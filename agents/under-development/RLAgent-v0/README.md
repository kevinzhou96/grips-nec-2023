RLAgent-v0: a first attempt at an RL training platform
==================================


RLAgent-v0 implements the following:

- `factory.py` - implements `FixedPartnerNumberWithOpponentsOneShotFactory`, which is a custom factory that allows one to specify possible opponents that can be included in generated worlds. This can be useful for training against agents that are known to have high performance.
- `observation.py` - implements several custom observation managers.
  - `BetterFixedPartnerNumbersObservationManager` : an observation manager that removes some extraneous information from and incorporates some additional relevant information of the state compared to the stock `FixedParterNumbersObservationManager`.
  - `DictBetterObservationManager` : an observation manager that uses a `Dict` observation space, allowing it to mix `Discrete` and `Box` observations. The `Dict` observation space also makes reading the specifications of the state much easier.
  - `FixedPartnerNumbersObservationManager_History` : an observation manager that tracks previous observations in order to allow the agent to also see prior offers when deciding on an action.
- `reward.py` - implements some custom reward functions.
  - `ReducingNeedsReward` : a custom reward function that not only measures profit/loss at the end of a day of negotiations but also rewards the agent for reducing needed quantity from one round of negotiations to the next
  - `QuantityBasedReward` : a custom reward function that only takes into account needed quantity and ignores profit. Could possible improve stability since profit is highly dependant on random world variables.
- `test.py` - implements two basic testing scripts
  - `test_tournament` : runs an RL agent in a tournament against the specified opponents. Useful for evaluating an agent in a competition setting against known good agents.
  - `test_models` : runs simulations to test a model or set of models utilizing a specified world factory. Useful in evaluating a model trained on worlds generated by a particular factory. 
- `train.py` : implements an easy-to-use training interface. Automatically creates a training environment and sets up checkpointing and TensorBoard logging. Many key hyperparameters can be set via command line, and any other hyperparameters can be passed to the environment, model, or training loop directly.
- `util.py` : implements some useful utility functions.